import sys
import os
import argparse
import yaml
import json
import logging as py_logging
from typing import Annotated
from pydantic import Field

# ãƒ­ã‚®ãƒ³ã‚°åˆæœŸåŒ–
py_logging.basicConfig(level=py_logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆ
try:
    from .recursive_thinking_ai import EnhancedRecursiveThinkingChat
    py_logging.debug("Imported EnhancedRecursiveThinkingChat via relative import")
except ImportError as e:
    py_logging.debug(f"Relative import failed: {e}, trying absolute import")
    try:
        # ç›´æŽ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆ
        from cort_mcp.recursive_thinking_ai import EnhancedRecursiveThinkingChat
        py_logging.debug("Imported EnhancedRecursiveThinkingChat via absolute import")
    except ImportError as e2:
        py_logging.debug(f"Absolute import failed: {e2}, trying sys.path modification")
        # é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚ŒãŸå ´åˆ
        src_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        py_logging.debug(f"Adding path to sys.path: {src_path}")
        sys.path.append(src_path)
        try:
            from recursive_thinking_ai import EnhancedRecursiveThinkingChat
            py_logging.debug("Imported EnhancedRecursiveThinkingChat via sys.path modification")
        except ImportError as e3:
            py_logging.error(f"All import attempts failed: {e3}")
            raise

# MCPã‚µãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from fastmcp import FastMCP
    py_logging.debug("Imported FastMCP from fastmcp package")
except ImportError as e:
    py_logging.debug(f"Import from fastmcp failed: {e}, trying mcp.server.fastmcp")
    try:
        from mcp.server.fastmcp import FastMCP
        py_logging.debug("Imported FastMCP from mcp.server.fastmcp")
    except ImportError as e2:
        py_logging.error(f"Failed to import FastMCP: {e2}")
        raise

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©
DEFAULT_MODEL = "mistralai/mistral-small-3.1-24b-instruct:free"
DEFAULT_PROVIDER = "openrouter"

# --- Logging Setup ---
def setup_logging(log: str, logfile: str):
    import logging
    import sys
    import os
    if log == "on":
        if not logfile or not logfile.startswith("/"):
            print("[FATAL] --logfile must be an absolute path when --log=on", file=sys.stderr)
            sys.exit(1)
        log_dir = os.path.dirname(logfile)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                print(f"[FATAL] Failed to create log directory: {log_dir} error={e}", file=sys.stderr)
                sys.exit(1)
        try:
            # --- Full handler initialization ---
            root_logger = logging.getLogger()
            for handler in root_logger.handlers[:]:
                root_logger.removeHandler(handler)
            root_logger.handlers.clear()
            root_logger.setLevel(logging.DEBUG)

            # Add only FileHandler to root logger
            file_handler = logging.FileHandler(logfile, mode="a", encoding="utf-8")
            file_handler.setLevel(logging.DEBUG)
            formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
            file_handler.setFormatter(formatter)
            root_logger.addHandler(file_handler)

            # Also add StreamHandler (stdout)
            stream_handler = logging.StreamHandler(sys.stdout)
            stream_handler.setLevel(logging.DEBUG)
            stream_handler.setFormatter(formatter)
            root_logger.addHandler(stream_handler)

            # Explicitly call flush
            file_handler.flush()

            # Set global logger as well
            specific_logger = logging.getLogger("cort-mcp-server")
            specific_logger.handlers.clear()  # Clear existing handlers
            specific_logger.setLevel(logging.DEBUG)
            specific_logger.addHandler(file_handler)
            specific_logger.addHandler(stream_handler)
            specific_logger.propagate = False

            specific_logger.debug(f"=== MCP Server log initialized: {logfile} ===")
            print(f"[INFO] MCP Server log initialized: {logfile}")
            if os.path.exists(logfile):
                print(f"[INFO] Log file created: {logfile}")
            else:
                print(f"[WARN] Log file NOT created: {logfile}")

            return specific_logger
        except Exception as e:
            print(f"[FATAL] Failed to create log file: {logfile} error={e}", file=sys.stderr)
            sys.exit(1)
    elif log == "off":
        # Completely disable logging functionality
        logging.disable(logging.CRITICAL)
        print("[INFO] Logging disabled (--log=off)")
        return None
    else:
        print("[FATAL] --log must be 'on' or 'off'", file=sys.stderr)
        sys.exit(1)

def resolve_model_and_provider(params):
    print("=== resolve_model_and_provider called ===")
    py_logging.info("=== resolve_model_and_provider called ===")
    import os
    # æ—¢å­˜ã®py_loggingï¼ˆlogging as py_loggingã§importæ¸ˆã¿ï¼‰ã‚’ä½¿ã†
    # ãƒ‡ãƒãƒƒã‚°: ç’°å¢ƒå¤‰æ•°ã®çŠ¶æ…‹ã‚’å‡ºåŠ›
    def mask_key(key):
        if key:
            return 'SET'
        return 'NOT_SET'
    py_logging.info(f"[DEBUG] ENV OPENROUTER_API_KEY={mask_key(os.getenv('OPENROUTER_API_KEY'))}")
    py_logging.info(f"[DEBUG] ENV OPENAI_API_KEY={mask_key(os.getenv('OPENAI_API_KEY'))}")
    # params: dict
    model = params.get("model")
    provider = params.get("provider")
    py_logging.info(f"[DEBUG] params: model={model}, provider={provider}")
    if not model:
        model = DEFAULT_MODEL
    if not provider:
        provider = DEFAULT_PROVIDER
    py_logging.info(f"[DEBUG] after default: model={model}, provider={provider}")
    # ã“ã“ã§APIã‚­ãƒ¼å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆproviderãŒä¸æ­£/æœªè¨­å®šå«ã‚€ï¼‰
    api_key = get_api_key(provider)
    py_logging.info(f"[DEBUG] get_api_key(provider={provider}) -> {mask_key(api_key)}")
    if not api_key:
        # providerãŒä¸æ­£ã€APIã‚­ãƒ¼ç„¡ã—â†’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        provider = DEFAULT_PROVIDER
        model = DEFAULT_MODEL
        api_key = get_api_key(provider)
        py_logging.info(f"[DEBUG] fallback: model={model}, provider={provider}, api_key={mask_key(api_key)}")
    # ã•ã‚‰ã«ã€Œãƒ¢ãƒ‡ãƒ«åãŒproviderã«å­˜åœ¨ã—ãªã„ã€ç­‰ã®ãƒã‚§ãƒƒã‚¯ã¯AIå´APIã§ä¾‹å¤–ç™ºç”Ÿæ™‚ã«æ¤œçŸ¥
    return model, provider, api_key

def get_api_key(provider):
    if provider == "openai":
        key = os.getenv("OPENAI_API_KEY")
    elif provider == "openrouter":
        key = os.getenv("OPENROUTER_API_KEY")
    else:
        key = None
    return key

# FastMCPã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
server = FastMCP(
    name="Chain-of-Recursive-Thoughts MCP Server",
    instructions="Provide deeper recursive thinking and reasoning for the given prompt. Use the MCP Server when you encounter complex problems.",
)

# Define tools using decorators
@server.tool(
    name="cort.think.simple",
    description="""
    Return a simple recursive thinking AI response.

    Parameters:
        prompt (str, required): Input prompt for the AI.
        model (str, optional): LLM model name. If not specified, uses default.
        provider (str, optional): API provider name. If not specified, uses default.

    Returns:
        dict: {
            "response": AI response (string),
            "model": model name used (string),
            "provider": provider name used (string)
        }

    Notes:
        - If model/provider is omitted, defaults are used.
        - Do not pass null or empty string for optional params.
        - See README for fallback logic on API errors.
    """
)
async def cort_think_simple(
    prompt: Annotated[str, Field(description="Input prompt for the AI (required)")],
    model: Annotated[str | None, Field(description="LLM model name. If not specified, uses default.")]=None,
    provider: Annotated[str | None, Field(description="API provider name. If not specified, uses default.")]=None
):
    resolved_model, resolved_provider, api_key = resolve_model_and_provider({"model": model, "provider": provider})
    py_logging.info(f"cort_think_simple called: prompt={prompt} model={resolved_model} provider={resolved_provider}")
    if not prompt:
        py_logging.warning("cort_think_simple: prompt is required")
        return {
            "error": "prompt is required"
        }
    try:
        chat = EnhancedRecursiveThinkingChat(api_key=api_key, model=resolved_model, provider=resolved_provider)
        result = chat.think(prompt, details=False)
        py_logging.info("cort_think_simple: result generated successfully")
        return {
            "response": result["response"],
            "model": resolved_model,
            "provider": resolved_provider
        }
    except Exception as e:
        py_logging.exception(f"[ERROR] cort_think_simple failed: {e}")
        fallback_api_key = get_api_key(DEFAULT_PROVIDER)
        if fallback_api_key:
            try:
                chat = EnhancedRecursiveThinkingChat(api_key=fallback_api_key, model=DEFAULT_MODEL, provider=DEFAULT_PROVIDER)
                result = chat.think(prompt, details=False)
                py_logging.info("cort_think_simple: fallback result generated successfully")
                return {
                    "response": result["response"],
                    "model": DEFAULT_MODEL,
                    "provider": f"{DEFAULT_PROVIDER} (fallback)"
                }
            except Exception as e2:
                py_logging.exception(f"[ERROR] cort_think_simple fallback also failed: {e2}")
                return {
                    "error": f"Failed to process request: {str(e)}. Fallback also failed: {str(e2)}"
                }
        else:
            py_logging.error("cort_think_simple: API key for OpenAI is missing (cannot fallback)")
            return {
                "error": f"Failed to process request: {str(e)}. API key for OpenAI is missing (cannot fallback)"
            }

@server.tool(
    name="cort.think.simple.neweval",
    description="""
    Return a simple recursive thinking AI response (new evaluation prompt version).

    Parameters:
        prompt (str, required): Input prompt for the AI.
        model (str, optional): LLM model name. If not specified, uses default.
        provider (str, optional): API provider name. If not specified, uses default.

    Returns:
        dict: {
            "response": AI response (string),
            "model": model name used (string),
            "provider": provider name used (string)
        }

    Notes:
        - If model/provider is omitted, defaults are used.
        - Do not pass null or empty string for optional params.
        - See README for fallback logic on API errors.
    """
)
async def cort_think_simple_neweval(
    prompt: Annotated[str, Field(description="Input prompt for the AI (required)")],
    model: Annotated[str | None, Field(description="LLM model name. If not specified, uses default.")]=None,
    provider: Annotated[str | None, Field(description="API provider name. If not specified, uses default.")]=None
):
    resolved_model, resolved_provider, api_key = resolve_model_and_provider({"model": model, "provider": provider})
    py_logging.info(f"cort_think_simple_neweval called: prompt={prompt} model={resolved_model} provider={resolved_provider}")
    if not prompt:
        py_logging.warning("cort_think_simple_neweval: prompt is required")
        return {
            "error": "prompt is required"
        }
    try:
        chat = EnhancedRecursiveThinkingChat(api_key=api_key, model=resolved_model, provider=resolved_provider)
        result = chat.think(prompt, details=False, neweval=True)
        py_logging.info("cort_think_simple_neweval: result generated successfully")
        return {
            "response": result["response"],
            "model": resolved_model,
            "provider": resolved_provider
        }
    except Exception as e:
        py_logging.exception(f"[ERROR] cort_think_simple_neweval failed: {e}")
        fallback_api_key = get_api_key(DEFAULT_PROVIDER)
        if fallback_api_key:
            try:
                chat = EnhancedRecursiveThinkingChat(api_key=fallback_api_key, model=DEFAULT_MODEL, provider=DEFAULT_PROVIDER)
                result = chat.think(prompt, details=False, neweval=True)
                py_logging.info("cort_think_simple_neweval: fallback result generated successfully")
                return {
                    "response": result["response"],
                    "model": DEFAULT_MODEL,
                    "provider": f"{DEFAULT_PROVIDER} (fallback)"
                }
            except Exception as e2:
                py_logging.exception(f"[ERROR] cort_think_simple_neweval fallback also failed: {e2}")
                return {
                    "error": f"Failed to process request: {str(e)}. Fallback also failed: {str(e2)}"
                }
        else:
            py_logging.error("cort_think_simple_neweval: API key for OpenAI is missing (cannot fallback)")
            return {
                "error": f"Failed to process request: {str(e)}. API key for OpenAI is missing (cannot fallback)"
            }

@server.tool(
    name="cort.think.details",
    description="""
    æ€è€ƒéŽç¨‹ã®è©³ç´°ã‚‚å«ã‚ã¦è¿”ã™å†å¸°çš„æ€è€ƒAIãƒ„ãƒ¼ãƒ«ã€‚

    Parameters:
        prompt (str, required): AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰ã€‚
        model (str, optional): åˆ©ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚æŒ‡å®šãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã€‚
        provider (str, optional): åˆ©ç”¨ã™ã‚‹APIãƒ—ãƒ­ãƒã‚¤ãƒ€åã€‚æŒ‡å®šãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’åˆ©ç”¨ã€‚

    Returns:
        dict: {
            "response": AIã®æœ€çµ‚å›žç­”ï¼ˆstringï¼‰, 
            "details": æ€è€ƒéŽç¨‹ã®å±¥æ­´ï¼ˆYAMLå½¢å¼, stringï¼‰, 
            "model": ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«åï¼ˆstringï¼‰, 
            "provider": ä½¿ç”¨ãƒ—ãƒ­ãƒã‚¤ãƒ€åï¼ˆstringï¼‰
        }

    Notes:
        - model/providerã‚’çœç•¥ã—ãŸå ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è‡ªå‹•é©ç”¨
        - ä¾‹å¤–ç™ºç”Ÿæ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿæ–½
        - æ€è€ƒå±¥æ­´ã¯YAMLå½¢å¼ã§detailsã‚­ãƒ¼ã«æ ¼ç´
    """
)
async def cort_think_details(
    prompt: Annotated[str, Field(description="AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰")],
    model: Annotated[str | None, Field(description="åˆ©ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã‚’æ­£ç¢ºã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n- æŽ¨å¥¨å€¤ï¼ˆOpenAIã®å ´åˆï¼‰: 'gpt-4.1-nano'\n- æŽ¨å¥¨å€¤ï¼ˆOpenRouterã®å ´åˆï¼‰: 'meta-llama/llama-4-maverick:free'\n- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: mistralai/mistral-small-3.1-24b-instruct:free\nãƒ¢ãƒ‡ãƒ«åã¯å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®å…¬å¼ãƒªã‚¹ãƒˆã«å¾“ã„ã€æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚æŒ‡å®šãŒãªã„å ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚")]=None,
    provider: Annotated[str | None, Field(description="åˆ©ç”¨ã™ã‚‹APIãƒ—ãƒ­ãƒã‚¤ãƒ€åã‚’æ­£ç¢ºã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n- æŒ‡å®šå¯èƒ½å€¤: 'openai' ã¾ãŸã¯ 'openrouter'\n- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€: openrouter\nãƒ—ãƒ­ãƒã‚¤ãƒ€ã«ã‚ˆã£ã¦é¸æŠžå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«åã¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®çµ„ã¿åˆã‚ã›ã«ã”æ³¨æ„ãã ã•ã„ã€‚æŒ‡å®šãŒãªã„å ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚")]=None
):
    resolved_model, resolved_provider, api_key = resolve_model_and_provider({"model": model, "provider": provider})
    py_logging.info(f"cort_think_details called: prompt={prompt} model={resolved_model} provider={resolved_provider}")
    if not prompt:
        py_logging.warning("cort_think_details: prompt is required")
        return {
            "error": "prompt is required"
        }
    try:
        chat = EnhancedRecursiveThinkingChat(api_key=api_key, model=resolved_model, provider=resolved_provider)
        result = chat.think(prompt, details=True)
        yaml_log = yaml.safe_dump({
            "thinking_rounds": result.get("thinking_rounds"),
            "thinking_history": result.get("thinking_history")
        }, allow_unicode=True, sort_keys=False)
        py_logging.info("cort_think_details: result generated successfully")
        return {
            "response": result["response"],
            "details": yaml_log,
            "model": resolved_model,
            "provider": resolved_provider
        }
    except Exception as e:
        py_logging.exception(f"[ERROR] cort_think_details failed: {e}")
        fallback_api_key = get_api_key(DEFAULT_PROVIDER)
        if fallback_api_key:
            try:
                chat = EnhancedRecursiveThinkingChat(api_key=fallback_api_key, model=DEFAULT_MODEL, provider=DEFAULT_PROVIDER)
                result = chat.think(prompt, details=True)
                yaml_log = yaml.safe_dump({
                    "thinking_rounds": result.get("thinking_rounds"),
                    "thinking_history": result.get("thinking_history")
                }, allow_unicode=True, sort_keys=False)
                py_logging.info("cort_think_details: fallback result generated successfully")
                return {
                    "response": result["response"],
                    "details": yaml_log,
                    "model": DEFAULT_MODEL,
                    "provider": f"{DEFAULT_PROVIDER} (fallback)"
                }
            except Exception as e2:
                py_logging.exception(f"[ERROR] cort_think_details fallback also failed: {e2}")
                return {
                    "error": f"Failed to process request: {str(e)}. Fallback also failed: {str(e2)}"
                }
        else:
            py_logging.error("cort_think_details: API key for OpenAI is missing (cannot fallback)")
            return {
                "error": f"Failed to process request: {str(e)}. API key for OpenAI is missing (cannot fallback)"
            }

@server.tool(
    name="cort.think.details.neweval",
    description="""
    æ€è€ƒéŽç¨‹ã®è©³ç´°ã‚‚å«ã‚ã¦è¿”ã™å†å¸°çš„æ€è€ƒAIãƒ„ãƒ¼ãƒ«ï¼ˆæ–°è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰ã€‚

    æ©Ÿèƒ½:
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã€å†å¸°çš„æ€è€ƒAIã®å¿œç­”ã¨ã€æ€è€ƒå±¥æ­´ã‚„éŽç¨‹ï¼ˆYAMLå½¢å¼ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        prompt (str, å¿…é ˆ): AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚
        model (str, ä»»æ„): åˆ©ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã‚’æ­£ç¢ºã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
    - æŽ¨å¥¨å€¤ï¼ˆOpenAIã®å ´åˆï¼‰: "gpt-4.1-nano"
    - æŽ¨å¥¨å€¤ï¼ˆOpenRouterã®å ´åˆï¼‰: "meta-llama/llama-4-maverick:free"
    - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: mistralai/mistral-small-3.1-24b-instruct:free
    ãƒ¢ãƒ‡ãƒ«åã¯å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®å…¬å¼ãƒªã‚¹ãƒˆã«å¾“ã„ã€æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚æŒ‡å®šãŒãªã„å ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚
        provider (str, ä»»æ„): åˆ©ç”¨ã™ã‚‹APIãƒ—ãƒ­ãƒã‚¤ãƒ€åã‚’æ­£ç¢ºã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
    - æŒ‡å®šå¯èƒ½å€¤: "openai" ã¾ãŸã¯ "openrouter"
    - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€: openrouter
    ãƒ—ãƒ­ãƒã‚¤ãƒ€ã«ã‚ˆã£ã¦é¸æŠžå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«åã¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®çµ„ã¿åˆã‚ã›ã«ã”æ³¨æ„ãã ã•ã„ã€‚æŒ‡å®šãŒãªã„å ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚

    æˆ»ã‚Šå€¤:
        dict: {
            "response": AIã®å¿œç­”ï¼ˆstringï¼‰, 
            "details": æ€è€ƒå±¥æ­´ã‚„éŽç¨‹ï¼ˆYAMLå½¢å¼ã®stringï¼‰, 
            "model": å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«åï¼ˆstringï¼‰, 
            "provider": å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åï¼ˆstringï¼‰
        }

    æ³¨æ„:
        - ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆmodel, providerï¼‰ã¯æœªæŒ‡å®šæ™‚ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã”ã¨çœç•¥ã—ã¦ãã ã•ã„ã€‚
        - æ˜Žç¤ºçš„ã«nullã‚„ç©ºæ–‡å­—ã‚’æ¸¡ã™ã¨APIå´ã§ã‚¨ãƒ©ãƒ¼ã¨ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
        - APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŒ™å‹•ã«ã¤ã„ã¦ã¯ã€README.md ã®ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®šã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
    """
)
async def cort_think_details_neweval(
    prompt: Annotated[str, Field(description="AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰")],
    model: Annotated[str | None, Field(description="åˆ©ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã‚’æ­£ç¢ºã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n- æŽ¨å¥¨å€¤ï¼ˆOpenAIã®å ´åˆï¼‰: 'gpt-4.1-nano'\n- æŽ¨å¥¨å€¤ï¼ˆOpenRouterã®å ´åˆï¼‰: 'meta-llama/llama-4-maverick:free'\n- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«: mistralai/mistral-small-3.1-24b-instruct:free\nãƒ¢ãƒ‡ãƒ«åã¯å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®å…¬å¼ãƒªã‚¹ãƒˆã«å¾“ã„ã€æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚æŒ‡å®šãŒãªã„å ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚")]=None,
    provider: Annotated[str | None, Field(description="åˆ©ç”¨ã™ã‚‹APIãƒ—ãƒ­ãƒã‚¤ãƒ€åã‚’æ­£ç¢ºã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n- æŒ‡å®šå¯èƒ½å€¤: 'openai' ã¾ãŸã¯ 'openrouter'\n- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€: openrouter\nãƒ—ãƒ­ãƒã‚¤ãƒ€ã«ã‚ˆã£ã¦é¸æŠžå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«åã¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®çµ„ã¿åˆã‚ã›ã«ã”æ³¨æ„ãã ã•ã„ã€‚æŒ‡å®šãŒãªã„å ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚")]=None
):
    resolved_model, resolved_provider, api_key = resolve_model_and_provider({"model": model, "provider": provider})
    py_logging.info(f"cort_think_details_neweval called: prompt={prompt} model={resolved_model} provider={resolved_provider}")
    if not prompt:
        py_logging.warning("cort_think_details_neweval: prompt is required")
        return {
            "error": "prompt is required"
        }
    try:
        chat = EnhancedRecursiveThinkingChat(api_key=api_key, model=resolved_model, provider=resolved_provider)
        result = chat.think(prompt, details=True, neweval=True)
        yaml_log = yaml.safe_dump({
            "thinking_rounds": result.get("thinking_rounds"),
            "thinking_history": result.get("thinking_history")
        }, allow_unicode=True, sort_keys=False)
        py_logging.info("cort_think_details_neweval: result generated successfully")
        return {
            "response": result["response"],
            "details": yaml_log,
            "model": resolved_model,
            "provider": resolved_provider
        }
    except Exception as e:
        py_logging.exception(f"[ERROR] cort_think_details_neweval failed: {e}")
        fallback_api_key = get_api_key(DEFAULT_PROVIDER)
        if fallback_api_key:
            try:
                chat = EnhancedRecursiveThinkingChat(api_key=fallback_api_key, model=DEFAULT_MODEL, provider=DEFAULT_PROVIDER)
                result = chat.think(prompt, details=True)
                yaml_log = yaml.safe_dump({
                    "thinking_rounds": result.get("thinking_rounds"),
                    "thinking_history": result.get("thinking_history")
                }, allow_unicode=True, sort_keys=False)
                py_logging.info("cort_think_details_neweval: fallback result generated successfully")
                return {
                    "response": result["response"],
                    "details": yaml_log,
                    "model": DEFAULT_MODEL,
                    "provider": f"{DEFAULT_PROVIDER} (fallback)"
                }
            except Exception as e2:
                py_logging.exception(f"[ERROR] cort_think_details_neweval fallback also failed: {e2}")
                return {
                    "error": f"Failed to process request: {str(e)}. Fallback also failed: {str(e2)}"
                }
        else:
            py_logging.error("cort_think_details_neweval: API key for OpenAI is missing (cannot fallback)")
            return {
                "error": f"Failed to process request: {str(e)}. API key for OpenAI is missing (cannot fallback)"
            }

# --- Mixed LLMãƒªã‚¹ãƒˆå®šç¾© ---
MIXED_LLM_LIST = [
    {"provider": "openai", "model": "gpt-4.1-mini"},
    {"provider": "openai", "model": "gpt-4.1-nano"},
    {"provider": "openai", "model": "gpt-4o-mini"},
    {"provider": "openrouter", "model": "meta-llama/llama-4-scout:free"},
    {"provider": "openrouter", "model": "google/gemini-2.0-flash-exp:free"},
    {"provider": "openrouter", "model": "mistralai/mistral-small-3.1-24b-instruct:free"},
    {"provider": "openrouter", "model": "google/gemma-3-27b-it:free"},
    {"provider": "openrouter", "model": "nousresearch/deephermes-3-mistral-24b-preview:free"},
]

def get_available_mixed_llms():
    """APIã‚­ãƒ¼ãŒæœ‰åŠ¹ãªã‚‚ã®ã ã‘è¿”ã™"""
    available = []
    for entry in MIXED_LLM_LIST:
        api_key = get_api_key(entry["provider"])
        if api_key:
            available.append({**entry, "api_key": api_key})
    return available

import random
from typing import Dict, Any

def generate_with_mixed_llm(prompt: str, details: bool = False, neweval: bool = False) -> Dict[str, Any]:
    available_llms = get_available_mixed_llms()
    if not prompt:
        py_logging.warning("mixed_llm: prompt is required")
        return {"error": "prompt is required"}
    if not available_llms:
        py_logging.error("mixed_llm: No available LLMs (API key missing)")
        return {"error": "No available LLMs (API key missing)"}

    # --- ãƒ©ã‚¦ãƒ³ãƒ‰æ•°ãƒ»æ¡ˆæ•°ã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã«å¾“ã„AIãŒæ±ºå®š ---
    # ã¾ãšãƒ™ãƒ¼ã‚¹LLMã‚’ãƒ©ãƒ³ãƒ€ãƒ ã§1ã¤é¸ã¶
    base_llm = random.choice(available_llms)
    chat = EnhancedRecursiveThinkingChat(api_key=base_llm["api_key"], model=base_llm["model"], provider=base_llm["provider"])
    # ãƒ™ãƒ¼ã‚¹å¿œç­”ç”Ÿæˆï¼ˆåˆå›žï¼‰
    thinking_rounds = chat._determine_thinking_rounds(prompt)
    py_logging.info("\n=== GENERATING INITIAL RESPONSE ===")
    py_logging.info(f"Base LLM: provider={base_llm['provider']}, model={base_llm['model']}, rounds={thinking_rounds}")
    base_response = chat._call_api([{"role": "user", "content": prompt}], temperature=0.7, stream=False)
    current_best = base_response
    py_logging.info("=" * 50)
    thinking_history = [{
        "round": 0,
        "llm_prompt": prompt,
        "llm_response": base_response,
        "response": base_response,
        "alternatives": [],
        "selected": -1,
        "explanation": "Initial base response",
        "provider": base_llm["provider"],
        "model": base_llm["model"]
    }]
    # Generate alternatives for each round
    # Use the same logic as EnhancedRecursiveThinkingChat.think (num_alternatives)
    num_alternatives = 3
    if hasattr(chat, 'num_alternatives'):
        num_alternatives = chat.num_alternatives
    for r in range(thinking_rounds):
        py_logging.info(f"\n=== ROUND {r+1}/{thinking_rounds} ===")
        alternatives = []
        alt_llm_info = []
        alt_llm_responses = []
        alt_llm_prompts = []
        for i in range(num_alternatives):
            py_logging.info(f"\nâœ¨ ALTERNATIVE {i+1} âœ¨")
            alt_llm = random.choice(available_llms)
            alt_prompt = f"""Original message: {prompt}\n\nCurrent response: {current_best}\n\nGenerate an alternative response that might be better. Be creative and consider different approaches.\nAlternative response:"""
            alt_messages = [{"role": "user", "content": alt_prompt}]
            alt_chat = EnhancedRecursiveThinkingChat(api_key=alt_llm["api_key"], model=alt_llm["model"], provider=alt_llm["provider"])
            alt_response = alt_chat._call_api(alt_messages, temperature=0.7 + i * 0.1, stream=False)
            py_logging.info(f"Alternative {i+1}: provider={alt_llm['provider']}, model={alt_llm['model']}")
            alternatives.append({
                "response": alt_response,
                "provider": alt_llm["provider"],
                "model": alt_llm["model"]
            })
            alt_llm_info.append({"provider": alt_llm["provider"], "model": alt_llm["model"]})
            alt_llm_responses.append(alt_response)
            alt_llm_prompts.append(alt_prompt)
        # è©•ä¾¡ã¯ãƒ™ãƒ¼ã‚¹LLMã§è¡Œã†ï¼ˆç¾çŠ¶CoRTã®æµå„€ã‚’è¸è¥²ï¼‰
        py_logging.info("\n=== EVALUATING RESPONSES ===")
        alts_text = "\n".join([f"{i+1}. {alt['response']}" for i, alt in enumerate(alternatives)])
        # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯AIã‚³ã‚¢å´ã§ä¸€å…ƒç®¡ç†
        eval_prompt = chat._build_eval_prompt(prompt, current_best, [alt['response'] for alt in alternatives], neweval=neweval)
        eval_messages = [{"role": "user", "content": eval_prompt}]
        evaluation = chat._call_api(eval_messages, temperature=0.2, stream=False)
        py_logging.info("=" * 50)

        lines = [line.strip() for line in evaluation.split('\n') if line.strip()]
        choice = 'current'
        explanation_text = "No explanation provided"
        if lines:
            first_line = lines[0].lower()
            if 'current' in first_line:
                choice = 'current'
            else:
                for char in first_line:
                    if char.isdigit():
                        choice = char
                        break
            if len(lines) > 1:
                explanation_text = ' '.join(lines[1:])
        if choice == 'current':
            selected_response = current_best
            selected_idx = -1
            py_logging.info(f"\n    âœ“ Kept current response: {explanation_text}")
        else:
            try:
                idx = int(choice) - 1
                if 0 <= idx < len(alternatives):
                    selected_response = alternatives[idx]["response"]
                    selected_idx = idx
                    py_logging.info(f"\n    âœ“ Selected alternative {idx+1}: {explanation_text}")
                else:
                    selected_response = current_best
                    selected_idx = -1
                    py_logging.info(f"\n    âœ“ Invalid selection, keeping current response")
            except Exception:
                selected_response = current_best
                selected_idx = -1
                py_logging.info(f"\n    âœ“ Could not parse selection, keeping current response")
        # é¸æŠžã•ã‚ŒãŸprovider/modelã‚’è¨˜éŒ²
        if selected_idx != -1 and 0 <= selected_idx < len(alternatives):
            sel_provider = alternatives[selected_idx]["provider"]
            sel_model = alternatives[selected_idx]["model"]
        else:
            # current_bestã¯base_llmã¾ãŸã¯å‰å›žã®best
            # ç›´å‰ã®thinking_historyã‹ã‚‰æ‹¾ã†ï¼ˆãªã‘ã‚Œã°base_llmï¼‰
            if thinking_history:
                sel_provider = thinking_history[-1].get("provider", base_llm["provider"])
                sel_model = thinking_history[-1].get("model", base_llm["model"])
            else:
                sel_provider = base_llm["provider"]
                sel_model = base_llm["model"]
        thinking_history.append({
            "round": r + 1,
            "llm_prompt": alt_llm_prompts,
            "llm_response": alt_llm_responses,
            "response": selected_response,
            "alternatives": alternatives,
            "selected": selected_idx,
            "explanation": explanation_text,
            "alternatives_llm": alt_llm_info,
            "provider": sel_provider,
            "model": sel_model
        })
        current_best = selected_response
    py_logging.info("\n" + "=" * 50)
    py_logging.info("ðŸŽ¯ FINAL RESPONSE SELECTED")
    py_logging.info("=" * 50)
    result = {"response": current_best}
    # detailsã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšã€æœ€ä½Žé™ã®ãƒ¡ã‚¿æƒ…å ±ã¯å¸¸ã«è¿”ã™
    result["thinking_rounds"] = thinking_rounds
    result["thinking_history"] = thinking_history
    # æœ€çµ‚å›žç­”ã‚’ç”Ÿæˆã—ãŸprovider/modelã‚’å¿…ãšbestã«æ ¼ç´ï¼ˆsimpleç”¨ï¼‰
    last_provider = None
    last_model = None
    if thinking_history and isinstance(thinking_history[-1], dict):
        last_provider = thinking_history[-1].get("provider")
        last_model = thinking_history[-1].get("model")
    # å¿µã®ãŸã‚nullå›žé¿
    if not last_provider or not last_model:
        # æœ€å¾Œã®alternativesã‹ã‚‰å–å¾—ï¼ˆé¸æŠžè‚¢ãŒã‚ã‚Œã°ï¼‰
        last_alts = thinking_history[-1].get("alternatives", [])
        if last_alts and isinstance(last_alts, list):
            last_alt = last_alts[-1]
            last_provider = last_provider or last_alt.get("provider")
            last_model = last_model or last_alt.get("model")
    # ãã‚Œã§ã‚‚ãªã‘ã‚Œã°base_llm
    if not last_provider:
        last_provider = base_llm["provider"]
    if not last_model:
        last_model = base_llm["model"]
    result["best"] = {
        "response": current_best,
        "provider": last_provider,
        "model": last_model
    }
    if details:
        # è©³ç´°ãƒ¢ãƒ¼ãƒ‰ã®ã¿è¿½åŠ æƒ…å ±
        result["alternatives"] = thinking_history[-1]["alternatives"] if thinking_history else []
    return result

# --- MCPãƒ„ãƒ¼ãƒ«å®šç¾© ---
from typing import Annotated
from pydantic import Field

@server.tool(
    name="cort.think.simple_mixed_llm",
    description="Generate recursive thinking AI response using a different LLM (provider/model) for each alternative. No history/details output. Parameters: prompt (str, required). model/provider cannot be specified (randomly selected internally). Provider/model info for each alternative is always logged and included in the output.",
)
async def cort_think_simple_mixed_llm(
    prompt: Annotated[str, Field(description="AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰")]
):
    result = generate_with_mixed_llm(prompt, details=False)
    # å¿…è¦ãªæƒ…å ±ã®ã¿æŠ½å‡º
    response = result.get("response")
    best = result.get("best")
    return {
        "response": response,
        "provider": best["provider"],
        "model": best["model"]
    }

@server.tool(
    name="cort.think.simple_mixed_llm.neweval",
    description="""
    Generate recursive thinking AI response using a different LLM (provider/model) for each alternative. No history/details output. (new evaluation prompt version)

    Parameters:
        prompt (str, required): AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰ã€‚
        model/provider cannot be specified (randomly selected internally)ã€‚
        Provider/model info for each alternative is always logged and included in the output.

    Returns:
        dict: {
            "response": AI response (string),
            "provider": provider name used (string),
            "model": model name used (string)
        }
    """
)
async def cort_think_simple_mixed_llm_neweval(
    prompt: Annotated[str, Field(description="AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰")]
):
    result = generate_with_mixed_llm(prompt, details=False, neweval=True)
    # newevalå°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€details=False, neweval=Trueã§thinkã‚’å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã“ã“ã§æ˜Žç¤º
    response = result.get("response")
    best = result.get("best")
    return {
        "response": response,
        "provider": best["provider"],
        "model": best["model"]
    }

@server.tool(
    name="cort.think.details_mixed_llm",
    description="Generate recursive thinking AI response with full history, using a different LLM (provider/model) for each alternative. Parameters: prompt (str, required). model/provider cannot be specified (randomly selected internally). Provider/model info for each alternative is always logged and included in the output and history.",
)
async def cort_think_details_mixed_llm(
    prompt: Annotated[str, Field(description="AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰")]
):
    result = generate_with_mixed_llm(prompt, details=True)
    import yaml
    if "thinking_rounds" in result and "thinking_history" in result:
        result["details"] = yaml.safe_dump({
            "thinking_rounds": result["thinking_rounds"],
            "thinking_history": result["thinking_history"]
        }, allow_unicode=True, sort_keys=False)
    return result

@server.tool(
    name="cort.think.details_mixed_llm.neweval",
    description="""
    Generate recursive thinking AI response with full history, using a different LLM (provider/model) for each alternative. (new evaluation prompt version)

    Parameters:
        prompt (str, required): AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰ã€‚
        model/provider cannot be specified (randomly selected internally)ã€‚
        Provider/model info for each alternative is always logged and included in the output and history.

    Returns:
        dict: {
            "response": AI response (string),
            "details": YAMLå½¢å¼ã®æ€è€ƒå±¥æ­´ (string),
            "thinking_rounds": int,
            "thinking_history": list,
            "best": dict,
            "alternatives": list (details=Trueæ™‚ã®ã¿)
        }
    """
)
async def cort_think_details_mixed_llm_neweval(
    prompt: Annotated[str, Field(description="AIã¸ã®å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå¿…é ˆï¼‰")]
):
    result = generate_with_mixed_llm(prompt, details=True, neweval=True)
    import yaml
    if "thinking_rounds" in result and "thinking_history" in result:
        result["details"] = yaml.safe_dump({
            "thinking_rounds": result["thinking_rounds"],
            "thinking_history": result["thinking_history"]
        }, allow_unicode=True, sort_keys=False)
    return result

# Tools are registered with decorators

def initialize_and_run_server():
    # Initialize and run the MCP server.
    import logging as py_logging
    py_logging.info("cort-mcp server starting...")
    # MCPã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œ
    server.run()

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Chain-of-Recursive-Thoughts MCP Server/CLI")
    parser.add_argument("--log", choices=["on", "off"], required=True, help="Enable or disable logging (on/off)")
    parser.add_argument("--logfile", type=str, required=False, help="Absolute path to log file (required if --log=on)")

    args = parser.parse_args()
    if args.log == "on" and not args.logfile:
        print("[FATAL] --logfile is required when --log=on", file=sys.stderr)
        sys.exit(1)
    if args.log == "on" and not args.logfile.startswith("/"):
        print("[FATAL] --logfile must be an absolute path when --log=on", file=sys.stderr)
        sys.exit(1)
    global py_logging
    logger = setup_logging(args.log, args.logfile)
    import logging as py_logging
    py_logging = logger
    py_logging.info("cort-mcp main() started")
    try:
        py_logging.info("Server mode: waiting for MCP stdio requests...")
        # FastMCPã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
        initialize_and_run_server()
    except Exception as e:
        py_logging.exception(f"[ERROR] main() failed: {e}")
        return 1

if __name__ == "__main__":
    main()
